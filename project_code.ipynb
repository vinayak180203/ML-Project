{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ty3gYQgtnwFA",
        "outputId": "adc11346-5a7f-427f-8ff1-cecc2843719a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Importing Google Drive in which datasets are stored\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nl2tU6kL8Ot3"
      },
      "outputs": [],
      "source": [
        "#Importing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "from scipy.sparse.linalg import svds\n",
        "import h5py\n",
        "from scipy.sparse import csc_matrix\n",
        "from time import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cq3KEUaVo1o3"
      },
      "outputs": [],
      "source": [
        "#Function to import MovieLens_100K dataset\n",
        "def load_data_100k(path='./', delimiter='\\t'):\n",
        "\n",
        "    #Loading ratings data\n",
        "    ratings_data = pd.read_csv('/content/drive/MyDrive/Datasets/ratings.csv')\n",
        "\n",
        "    #Creating dictionaries to map original user and movie IDs to zero-based indices\n",
        "    user_id_to_index = {user_id: i for i, user_id in enumerate(ratings_data['userId'].unique())}\n",
        "    movie_id_to_index = {movie_id: i for i, movie_id in enumerate(ratings_data['movieId'].unique())}\n",
        "\n",
        "    num_users = len(user_id_to_index)  #Calculating number of users\n",
        "    num_movies = len(movie_id_to_index)  #Calculating number of movies\n",
        "\n",
        "    #Splitting the data into training and test sets\n",
        "    train_data, test_data = train_test_split(ratings_data, test_size=0.2, random_state=42)\n",
        "\n",
        "    #Initialization of train_ratings and test_ratings as two-dimensional arrays filled with zeros\n",
        "    train_ratings = np.zeros((num_movies, num_users), dtype='float32')\n",
        "    test_ratings = np.zeros((num_movies, num_users), dtype='float32')\n",
        "\n",
        "    #Extraction of user, movie and ratings data from each data point in training dataset and and this extracted rating is stored in train_ratings matrix at corresponding user-movie position\n",
        "    for index, row in train_data.iterrows():\n",
        "        user_id = user_id_to_index[row['userId']]\n",
        "        movie_id = movie_id_to_index[row['movieId']]\n",
        "        rating = row['rating']\n",
        "\n",
        "        train_ratings[movie_id, user_id] = rating\n",
        "\n",
        "    #Extraction of user, movie and ratings data from each data point in test dataset and and this extracted rating is stored in test_ratings matrix at corresponding user-movie position\n",
        "    for index, row in test_data.iterrows():\n",
        "        user_id = user_id_to_index[row['userId']]\n",
        "        movie_id = movie_id_to_index[row['movieId']]\n",
        "        rating = row['rating']\n",
        "\n",
        "        test_ratings[movie_id, user_id] = rating\n",
        "\n",
        "    #Creating of binary masks for training and test datasets where 0 in this mask represents no rating and 1 in this mask reprsents that a rating exists\n",
        "    train_masks = np.greater(train_ratings, 1e-12).astype('float32')\n",
        "    test_masks = np.greater(test_ratings, 1e-12).astype('float32')\n",
        "\n",
        "    #Displaying confirmating of datasets being loaded in matrices, number of users, number of movies, number of training ratings and number of test ratings\n",
        "    print('Data matrix loaded')\n",
        "    print('Number of users: {}'.format(num_users))\n",
        "    print('Number of movies: {}'.format(num_movies))\n",
        "    print('Number of training ratings:', train_data.shape[0])\n",
        "    print('Number of test ratings:', test_data.shape[0])\n",
        "\n",
        "    #Returning number of movies value, number of users value, train_ratings, test_ratings matrices and the two binary masks created above\n",
        "    return num_movies, num_users, train_ratings, train_masks, test_ratings, test_masks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data\n",
        "path = '/content/drive/MyDrive/Datasets/MovieLens_100K/'\n",
        "n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wt-URsxKTOb",
        "outputId": "cfa29220-2417-49e2-d8af-5242f98ae6a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data matrix loaded\n",
            "Number of users: 610\n",
            "Number of movies: 9724\n",
            "Number of training ratings: 80668\n",
            "Number of test ratings: 20168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Training the Basic Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Separating Validation Data from Training Dataset\n",
        "train_r, val_r, train_m, val_m = train_test_split(train_r, train_m, test_size=0.2, random_state=42)\n",
        "\n",
        "class MatrixFactorization:\n",
        "    #Initialization of class MatrixFactorization with hyperparameters number of latent factors, learning rate and number of training epochs\n",
        "    def __init__(self, n_factors=10, learning_rate=0.01, num_epochs=100):\n",
        "        self.n_factors = n_factors\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    def fit(self, train_r, train_m):\n",
        "        #Declaring self.n_users and self.n_items from the shape of train_r\n",
        "        self.n_users, self.n_items = train_r.shape\n",
        "        #Initialization of two matrices with random values which represent latent factors for users and items respectively\n",
        "        self.P = np.random.rand(self.n_users, self.n_factors)\n",
        "        self.Q = np.random.rand(self.n_items, self.n_factors)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for i in range(self.n_users):\n",
        "                for j in range(self.n_items):\n",
        "                    if train_m[i, j] == 1:  #Checking if rating exists at ith row and jth column by looking in binary mask matrix\n",
        "                        eij = train_r[i, j] - np.dot(self.P[i, :], self.Q[j, :]) #Calculating the prediction error\n",
        "                        #Updating the latent factors\n",
        "                        for k in range(self.n_factors):\n",
        "                            self.P[i, k] += self.learning_rate * (2 * eij * self.Q[j, k])\n",
        "                            self.Q[j, k] += self.learning_rate * (2 * eij * self.P[i, k])\n",
        "\n",
        "    #Function to predict the ratings\n",
        "    def predict(self, data):\n",
        "        user_indices, item_indices = data[:, 0].astype(int), data[:, 1].astype(int)\n",
        "        predictions = np.dot(self.P, self.Q.T)\n",
        "        return predictions[user_indices, item_indices]"
      ],
      "metadata": {
        "id": "KlZsrlyI7TGn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the matrix factorization model and fitting it on the training data\n",
        "model = MatrixFactorization(n_factors=10, learning_rate=0.01, num_epochs=20)\n",
        "model.fit(train_r, train_m)"
      ],
      "metadata": {
        "id": "YeLjOVik8G99"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "6V8wbZid7L6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Making predictions on the validation and test datasets\n",
        "val_pred = model.predict(val_r)\n",
        "test_pred = model.predict(test_r)\n",
        "\n",
        "#Defining a threshold to classify as like or not like\n",
        "threshold = 3.5\n",
        "\n",
        "#Initializing variables to keep track of correct and total predictions\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "#Initialization of empty val_errors array to store validation dataset errors for each user-item pair\n",
        "val_errors = []\n",
        "for i in range(val_r.shape[0]):\n",
        "    for j in range(val_r.shape[1]):\n",
        "        if val_m[i, j]: #Checking if rating exists at ith row and jth column by looking in binary mask matrix\n",
        "            actual_rating = val_r[i, j]\n",
        "            predicted_rating = val_pred[i]\n",
        "            val_errors.append((actual_rating - predicted_rating) ** 2) #Calculating the squared error between actual and predicted rating\n",
        "            #Classifying the predicted rating as like (1) or not like (0) based on the threshold\n",
        "            predicted_label = 1 if predicted_rating >= threshold else 0\n",
        "\n",
        "            #Comparing the actual label with the predicted label\n",
        "            if actual_rating >= threshold and predicted_label == 1:\n",
        "                correct_predictions += 1\n",
        "            elif actual_rating < threshold and predicted_label == 0:\n",
        "                correct_predictions += 1\n",
        "\n",
        "            total_predictions += 1\n",
        "\n",
        "#Calculating the validation accuracy\n",
        "validation_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "val_rmse = np.sqrt(np.mean(val_errors)) #Calcualting RMSE for validation dataset\n",
        "\n",
        "#Initialization of empty test_errors array to store validation dataset errors for each user-item pair\n",
        "test_errors = []\n",
        "for i in range(test_r.shape[0]):\n",
        "    for j in range(test_r.shape[1]):\n",
        "        if test_m[i, j]: #Checking if rating exists at ith row and jth column by looking in binary mask matrix\n",
        "            actual_rating = test_r[i, j]\n",
        "            predicted_rating = test_pred[i]\n",
        "            test_errors.append((actual_rating - predicted_rating) ** 2) #Calculating the squared error between actual and predicted rating\n",
        "\n",
        "test_rmse = np.sqrt(np.mean(test_errors)) #Calcualting RMSE for test dataset\n",
        "\n",
        "#Displaying validation accuracy, validation RMSE and test RMSE\n",
        "print(\"Validation Accuracy:\", validation_accuracy)\n",
        "print(\"Validation RMSE:\", val_rmse)\n",
        "print(\"Test RMSE:\", test_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Crs481jxi5D-",
        "outputId": "54ed73c5-dc50-420e-e17a-2e372a3f77ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5713220212567592\n",
            "Validation RMSE: 1.6038169957528507\n",
            "Test RMSE: 1.6039513725621861\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}