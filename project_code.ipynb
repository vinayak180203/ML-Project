{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "ty3gYQgtnwFA",
        "outputId": "b2e5cecc-2842-4e1e-9132-23cf0a1c8c87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Importing Google Drive in which datasets are stored\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "nl2tU6kL8Ot3"
      },
      "outputs": [],
      "source": [
        "#Importing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "from scipy.sparse.linalg import svds\n",
        "import h5py\n",
        "from scipy.sparse import csc_matrix\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "cq3KEUaVo1o3"
      },
      "outputs": [],
      "source": [
        "#Function to import MovieLens_100K dataset\n",
        "def load_data_100k(path='./', delimiter='\\t'):\n",
        "\n",
        "    #Loading ratings data\n",
        "    ratings_data = pd.read_csv('/content/drive/MyDrive/Datasets/ratings.csv')\n",
        "\n",
        "    #Creating dictionaries to map original user and movie IDs to zero-based indices\n",
        "    user_id_to_index = {user_id: i for i, user_id in enumerate(ratings_data['userId'].unique())}\n",
        "    movie_id_to_index = {movie_id: i for i, movie_id in enumerate(ratings_data['movieId'].unique())}\n",
        "\n",
        "    num_users = len(user_id_to_index)  #Calculating number of users\n",
        "    num_movies = len(movie_id_to_index)  #Calculating number of movies\n",
        "\n",
        "    #Splitting the data into training and test sets\n",
        "    train_data, test_data = train_test_split(ratings_data, test_size=0.2, random_state=42)\n",
        "\n",
        "    #Initialization of train_ratings and test_ratings as two-dimensional arrays filled with zeros\n",
        "    train_ratings = np.zeros((num_movies, num_users), dtype='float32')\n",
        "    test_ratings = np.zeros((num_movies, num_users), dtype='float32')\n",
        "\n",
        "    #Extraction of user, movie and ratings data from each data point in training dataset and and this extracted rating is stored in train_ratings matrix at corresponding user-movie position\n",
        "    for index, row in train_data.iterrows():\n",
        "        user_id = user_id_to_index[row['userId']]\n",
        "        movie_id = movie_id_to_index[row['movieId']]\n",
        "        rating = row['rating']\n",
        "\n",
        "        train_ratings[movie_id, user_id] = rating\n",
        "\n",
        "    #Extraction of user, movie and ratings data from each data point in test dataset and and this extracted rating is stored in test_ratings matrix at corresponding user-movie position\n",
        "    for index, row in test_data.iterrows():\n",
        "        user_id = user_id_to_index[row['userId']]\n",
        "        movie_id = movie_id_to_index[row['movieId']]\n",
        "        rating = row['rating']\n",
        "\n",
        "        test_ratings[movie_id, user_id] = rating\n",
        "\n",
        "    #Creating of binary masks for training and test datasets where 0 in this mask represents no rating and 1 in this mask reprsents that a rating exists\n",
        "    train_masks = np.greater(train_ratings, 1e-12).astype('float32')\n",
        "    test_masks = np.greater(test_ratings, 1e-12).astype('float32')\n",
        "\n",
        "    #Displaying confirmating of datasets being loaded in matrices, number of users, number of movies, number of training ratings and number of test ratings\n",
        "    print('Data matrix loaded')\n",
        "    print('Number of users: {}'.format(num_users))\n",
        "    print('Number of movies: {}'.format(num_movies))\n",
        "    print('Number of training ratings:', train_data.shape[0])\n",
        "    print('Number of test ratings:', test_data.shape[0])\n",
        "\n",
        "    #Returning number of movies value, number of users value, train_ratings, test_ratings matrices and the two binary masks created above\n",
        "    return num_movies, num_users, train_ratings, train_masks, test_ratings, test_masks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data\n",
        "path = '/content/drive/MyDrive/Datasets/MovieLens_100K/'\n",
        "n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wt-URsxKTOb",
        "outputId": "b2fdf117-982c-49ef-aa4a-6331bdeef44f"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data matrix loaded\n",
            "Number of users: 610\n",
            "Number of movies: 9724\n",
            "Number of training ratings: 80668\n",
            "Number of test ratings: 20168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Training the Basic Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Separating Validation Data from Training Dataset\n",
        "train_r, val_r, train_m, val_m = train_test_split(train_r, train_m, test_size=0.2, random_state=42)\n",
        "\n",
        "class MatrixFactorization:\n",
        "    #Initialization of class MatrixFactorization with hyperparameters number of latent factors, learning rate and number of training epochs\n",
        "    def __init__(self, n_factors=10, learning_rate=0.01, num_epochs=100):\n",
        "        self.n_factors = n_factors\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    def fit(self, train_r, train_m):\n",
        "        #Declaring self.n_users and self.n_items from the shape of train_r\n",
        "        self.n_users, self.n_items = train_r.shape\n",
        "        #Initialization of two matrices with random values which represent latent factors for users and items respectively\n",
        "        self.P = np.random.rand(self.n_users, self.n_factors)\n",
        "        self.Q = np.random.rand(self.n_items, self.n_factors)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for i in range(self.n_users):\n",
        "                for j in range(self.n_items):\n",
        "                    if train_m[i, j] == 1:  #Checking if rating exists at ith row and jth column by looking in binary mask matrix\n",
        "                        eij = train_r[i, j] - np.dot(self.P[i, :], self.Q[j, :]) #Calculating the prediction error\n",
        "                        #Updating the latent factors\n",
        "                        for k in range(self.n_factors):\n",
        "                            self.P[i, k] += self.learning_rate * (2 * eij * self.Q[j, k])\n",
        "                            self.Q[j, k] += self.learning_rate * (2 * eij * self.P[i, k])\n",
        "\n",
        "    #Function to predict the ratings\n",
        "    def predict(self, data):\n",
        "        user_indices, item_indices = data[:, 0].astype(int), data[:, 1].astype(int)\n",
        "        predictions = np.dot(self.P, self.Q.T)\n",
        "        return predictions[user_indices, item_indices]"
      ],
      "metadata": {
        "id": "KlZsrlyI7TGn"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the matrix factorization model and fitting it on the training data\n",
        "model = MatrixFactorization(n_factors=10, learning_rate=0.01, num_epochs=20)\n",
        "model.fit(train_r, train_m)"
      ],
      "metadata": {
        "id": "YeLjOVik8G99"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "6V8wbZid7L6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Making predictions on the validation and test datasets\n",
        "val_pred = model.predict(val_r)\n",
        "test_pred = model.predict(test_r)\n",
        "\n",
        "#Initialization of empty val_errors array to store validation dataset errors for each user-item pair\n",
        "val_errors = []\n",
        "for i in range(val_r.shape[0]):\n",
        "    for j in range(val_r.shape[1]):\n",
        "        if val_m[i, j]: #Checking if rating exists at ith row and jth column by looking in binary mask matrix\n",
        "            actual_rating = val_r[i, j]\n",
        "            predicted_rating = val_pred[i]\n",
        "            val_errors.append((actual_rating - predicted_rating) ** 2) #Calculating the squared error between actual and predicted rating\n",
        "\n",
        "val_rmse = np.sqrt(np.mean(val_errors)) #Calcualting RMSE for validation dataset\n",
        "\n",
        "#Initialization of empty test_errors array to store validation dataset errors for each user-item pair\n",
        "test_errors = []\n",
        "for i in range(test_r.shape[0]):\n",
        "    for j in range(test_r.shape[1]):\n",
        "        if test_m[i, j]: #Checking if rating exists at ith row and jth column by looking in binary mask matrix\n",
        "            actual_rating = test_r[i, j]\n",
        "            predicted_rating = test_pred[i]\n",
        "            test_errors.append((actual_rating - predicted_rating) ** 2) #Calculating the squared error between actual and predicted rating\n",
        "\n",
        "test_rmse = np.sqrt(np.mean(test_errors)) #Calcualting RMSE for test dataset\n",
        "\n",
        "#Displaying validation RMSE and test RMSE\n",
        "print(\"Validation RMSE:\", val_rmse)\n",
        "print(\"Test RMSE:\", test_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Crs481jxi5D-",
        "outputId": "c1cc0fba-e2b0-4fb0-98f9-55be7b93e062"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE: 1.331872049365042\n",
            "Test RMSE: 1.3290361933881936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is a basic model for movie recommendation with validation and data pipeline. In the next phase, we will try to implement another model with metadata of movies and try to make recommendation explainable. Also, if time permits and if we are able to get social network data, we will try to incorporate that also in our model."
      ],
      "metadata": {
        "id": "b0f7YvADyNB7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}