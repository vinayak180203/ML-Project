{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ty3gYQgtnwFA",
        "outputId": "79ebe409-46df-4c3a-e05f-f2442a046deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Importing Google Drive in which datasets are stored\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nl2tU6kL8Ot3"
      },
      "outputs": [],
      "source": [
        "#Importing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Concatenate, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the data\n",
        "ratings_data = pd.read_csv('/content/drive/MyDrive/Datasets/ratings.csv')\n",
        "movies_data = pd.read_csv('/content/drive/MyDrive/Datasets/movies.csv')\n",
        "ratings_data = pd.merge(ratings_data, movies_data[['movieId', 'genres']], on='movieId', how='left')\n",
        "\n",
        "#Encoding the labels for user IDs, movie IDs and movie genres\n",
        "user_encoder = LabelEncoder()\n",
        "movie_encoder = LabelEncoder()\n",
        "genre_encoder = LabelEncoder()\n",
        "ratings_data['userId'] = user_encoder.fit_transform(ratings_data['userId'])\n",
        "ratings_data['movieId'] = movie_encoder.fit_transform(ratings_data['movieId'])\n",
        "ratings_data['genres'] = genre_encoder.fit_transform(ratings_data['genres'])"
      ],
      "metadata": {
        "id": "1xqb7wuFG6GS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cq3KEUaVo1o3"
      },
      "outputs": [],
      "source": [
        "#Separating first 25,00 rows of dataframe to train Matrix Factorisation Model\n",
        "ratings_data2 = ratings_data.head(25000)\n",
        "\n",
        "#Function to pre-process data for Matrix Factorisation Model\n",
        "def load_data_100k(path='./', delimiter='\\t'):\n",
        "\n",
        "    #Creating dictionaries to map original user, movie IDs and genres to zero-based indices\n",
        "    user_id_to_index = {user_id: i for i, user_id in enumerate(ratings_data2['userId'].unique())}\n",
        "    movie_id_to_index = {movie_id: i for i, movie_id in enumerate(ratings_data2['movieId'].unique())}\n",
        "    genre_id_to_index = {genre_id: i for i, genre_id in enumerate(ratings_data2['genres'].unique())}\n",
        "\n",
        "    num_users = len(user_id_to_index)  #Calculating number of users\n",
        "    num_movies = len(movie_id_to_index)  #Calculating number of movies\n",
        "    num_genres = len(genre_id_to_index) #Calculating number of genres\n",
        "\n",
        "    #Splitting the training and test data\n",
        "    train_data, test_data = train_test_split(ratings_data2, test_size=0.2, random_state=24)\n",
        "\n",
        "    #Initialization of train_ratings and test_ratings as three-dimensional arrays filled with zeros\n",
        "    train_ratings = np.zeros((num_movies, num_users, num_genres), dtype='float32')\n",
        "    test_ratings = np.zeros((num_movies, num_users, num_genres), dtype='float32')\n",
        "\n",
        "    #Extraction of user, movie, genres and ratings data from each data point in training dataset and and this extracted rating is stored in train_ratings matrix at corresponding user-movie-genres position\n",
        "    for index, row in train_data.iterrows():\n",
        "        user_id = user_id_to_index[row['userId']]\n",
        "        movie_id = movie_id_to_index[row['movieId']]\n",
        "        genre_id = genre_id_to_index[row['genres']]\n",
        "        rating = row['rating']\n",
        "\n",
        "        train_ratings[movie_id, user_id] = rating\n",
        "\n",
        "    #Extraction of user, movie, genres and ratings data from each data point in test dataset and and this extracted rating is stored in test_ratings matrix at corresponding user-movie-genres position\n",
        "    for index, row in test_data.iterrows():\n",
        "        user_id = user_id_to_index[row['userId']]\n",
        "        movie_id = movie_id_to_index[row['movieId']]\n",
        "        genre_id = genre_id_to_index[row['genres']]\n",
        "        rating = row['rating']\n",
        "\n",
        "        test_ratings[movie_id, user_id] = rating\n",
        "\n",
        "    #Creating of binary masks for training and test datasets where 0 in this mask represents no rating and 1 in this mask reprsents that a rating exists\n",
        "    train_masks = np.greater(train_ratings, 1e-12).astype('float32')\n",
        "    test_masks = np.greater(test_ratings, 1e-12).astype('float32')\n",
        "\n",
        "    #Displaying confirmating of datasets being loaded in matrices, number of users, number of movies, number of training ratings and number of test ratings\n",
        "    print('Data matrix loaded')\n",
        "    print('Number of users: {}'.format(num_users))\n",
        "    print('Number of movies: {}'.format(num_movies))\n",
        "    print('Number of training ratings:', train_data.shape[0])\n",
        "    print('Number of test ratings:', test_data.shape[0])\n",
        "\n",
        "    #Returning number of movies value, number of users value, train_ratings, test_ratings matrices, the two binary masks and training and test data created above\n",
        "    return num_movies, num_users, num_genres, train_ratings, train_masks, test_ratings, test_masks, train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data\n",
        "path = '/content/drive/MyDrive/Datasets/MovieLens_100K/'\n",
        "n_m, n_u, n_g, train_r, train_m, test_r, test_m, train_data2, test_data2 = load_data_100k(path=path, delimiter='\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wt-URsxKTOb",
        "outputId": "e017df78-4ea5-41e9-9a81-e4ce48ad0f87"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data matrix loaded\n",
            "Number of users: 177\n",
            "Number of movies: 5194\n",
            "Number of training ratings: 20000\n",
            "Number of test ratings: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Training the Neural Network and Matrix Factorisation Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the training and test data for neural network model\n",
        "train_data, test_data = train_test_split(ratings_data, test_size=0.2, random_state=24)\n",
        "\n",
        "#To ensure that rating is of type category\n",
        "train_data['rating'] = train_data['rating'].astype('category')\n",
        "\n",
        "#Creating the neural network collaborative filtering model\n",
        "def create_model():\n",
        "    #Defining the input layers\n",
        "    user_input = Input(shape=(1,))\n",
        "    movie_input = Input(shape=(1,))\n",
        "    genre_input = Input(shape=(1,))\n",
        "\n",
        "    #Defining embedding layers\n",
        "    user_embedding = Embedding(input_dim=len(user_encoder.classes_), output_dim=20)(user_input)\n",
        "    movie_embedding = Embedding(input_dim=len(movie_encoder.classes_), output_dim=20)(movie_input)\n",
        "    genre_embedding = Embedding(input_dim=len(genre_encoder.classes_), output_dim=5)(genre_input)\n",
        "\n",
        "    #Flattening the embedding layers\n",
        "    user_flatten = Flatten()(user_embedding)\n",
        "    movie_flatten = Flatten()(movie_embedding)\n",
        "    genre_flatten = Flatten()(genre_embedding)\n",
        "\n",
        "    #Concatenating the flattened embeddings\n",
        "    concatenated = Concatenate()([user_flatten, movie_flatten, genre_flatten])\n",
        "\n",
        "    #Defining the dense layers\n",
        "    dense1 = Dense(128, activation='tanh')(concatenated)\n",
        "    dense1 = BatchNormalization()(dense1)\n",
        "    dense1 = Dropout(0.5)(dense1)\n",
        "\n",
        "    dense2 = Dense(64, activation='sigmoid')(dense1)\n",
        "    dense2 = BatchNormalization()(dense2)\n",
        "    dense2 = Dropout(0.3)(dense2)\n",
        "\n",
        "    dense3 = Dense(64, activation='relu')(dense2)\n",
        "    dense3 = BatchNormalization()(dense3)\n",
        "    dense3 = Dropout(0.3)(dense3)\n",
        "\n",
        "    #Output layer for classification\n",
        "    output = Dense(len(train_data['rating'].cat.categories), activation='softmax')(dense2)\n",
        "\n",
        "    #Creating and compiling the model\n",
        "    model = Model(inputs=[user_input, movie_input, genre_input], outputs=output)\n",
        "    model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "#Instantiating the neural network model\n",
        "model = create_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXsurzMyH-0R",
        "outputId": "ac9358ff-330f-4c53-f5b2-31a44c0daa65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the neural network model\n",
        "model.fit([train_data['userId'], train_data['movieId'], train_data['genres']],\n",
        "                         train_data['rating'].cat.codes,\n",
        "                         epochs=50, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ojPL_YZKJZr",
        "outputId": "8fb0a08f-e785-40c7-f8cf-2b26bdf4571f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1009/1009 [==============================] - 10s 8ms/step - loss: 2.0033 - accuracy: 0.2582 - val_loss: 1.7090 - val_accuracy: 0.3116\n",
            "Epoch 2/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.6871 - accuracy: 0.3377 - val_loss: 1.6656 - val_accuracy: 0.3358\n",
            "Epoch 3/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.6076 - accuracy: 0.3679 - val_loss: 1.6628 - val_accuracy: 0.3462\n",
            "Epoch 4/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.5522 - accuracy: 0.3950 - val_loss: 1.6763 - val_accuracy: 0.3351\n",
            "Epoch 5/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.5051 - accuracy: 0.4188 - val_loss: 1.6928 - val_accuracy: 0.3349\n",
            "Epoch 6/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.4664 - accuracy: 0.4327 - val_loss: 1.7213 - val_accuracy: 0.3298\n",
            "Epoch 7/50\n",
            "1009/1009 [==============================] - 6s 6ms/step - loss: 1.4377 - accuracy: 0.4422 - val_loss: 1.7435 - val_accuracy: 0.3283\n",
            "Epoch 8/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.4162 - accuracy: 0.4481 - val_loss: 1.7631 - val_accuracy: 0.3292\n",
            "Epoch 9/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.3972 - accuracy: 0.4535 - val_loss: 1.7696 - val_accuracy: 0.3318\n",
            "Epoch 10/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.3847 - accuracy: 0.4547 - val_loss: 1.7804 - val_accuracy: 0.3271\n",
            "Epoch 11/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.3664 - accuracy: 0.4610 - val_loss: 1.7961 - val_accuracy: 0.3290\n",
            "Epoch 12/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.3547 - accuracy: 0.4656 - val_loss: 1.8136 - val_accuracy: 0.3353\n",
            "Epoch 13/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.3414 - accuracy: 0.4714 - val_loss: 1.8122 - val_accuracy: 0.3276\n",
            "Epoch 14/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.3315 - accuracy: 0.4713 - val_loss: 1.8388 - val_accuracy: 0.3306\n",
            "Epoch 15/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.3202 - accuracy: 0.4750 - val_loss: 1.8337 - val_accuracy: 0.3317\n",
            "Epoch 16/50\n",
            "1009/1009 [==============================] - 6s 6ms/step - loss: 1.3106 - accuracy: 0.4796 - val_loss: 1.8615 - val_accuracy: 0.3248\n",
            "Epoch 17/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.3052 - accuracy: 0.4794 - val_loss: 1.8717 - val_accuracy: 0.3266\n",
            "Epoch 18/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.2911 - accuracy: 0.4891 - val_loss: 1.8856 - val_accuracy: 0.3284\n",
            "Epoch 19/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.2903 - accuracy: 0.4856 - val_loss: 1.8872 - val_accuracy: 0.3280\n",
            "Epoch 20/50\n",
            "1009/1009 [==============================] - 7s 6ms/step - loss: 1.2792 - accuracy: 0.4916 - val_loss: 1.9023 - val_accuracy: 0.3304\n",
            "Epoch 21/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.2746 - accuracy: 0.4924 - val_loss: 1.9064 - val_accuracy: 0.3268\n",
            "Epoch 22/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.2643 - accuracy: 0.4954 - val_loss: 1.9094 - val_accuracy: 0.3263\n",
            "Epoch 23/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.2607 - accuracy: 0.4940 - val_loss: 1.9282 - val_accuracy: 0.3232\n",
            "Epoch 24/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.2537 - accuracy: 0.5005 - val_loss: 1.9277 - val_accuracy: 0.3233\n",
            "Epoch 25/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.2492 - accuracy: 0.4994 - val_loss: 1.9474 - val_accuracy: 0.3233\n",
            "Epoch 26/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.2402 - accuracy: 0.5036 - val_loss: 1.9472 - val_accuracy: 0.3236\n",
            "Epoch 27/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.2347 - accuracy: 0.5049 - val_loss: 1.9543 - val_accuracy: 0.3240\n",
            "Epoch 28/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.2282 - accuracy: 0.5091 - val_loss: 1.9568 - val_accuracy: 0.3239\n",
            "Epoch 29/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.2219 - accuracy: 0.5105 - val_loss: 1.9738 - val_accuracy: 0.3214\n",
            "Epoch 30/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.2161 - accuracy: 0.5135 - val_loss: 1.9879 - val_accuracy: 0.3220\n",
            "Epoch 31/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.2112 - accuracy: 0.5176 - val_loss: 2.0085 - val_accuracy: 0.3192\n",
            "Epoch 32/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.2009 - accuracy: 0.5204 - val_loss: 1.9997 - val_accuracy: 0.3202\n",
            "Epoch 33/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.1978 - accuracy: 0.5210 - val_loss: 2.0102 - val_accuracy: 0.3168\n",
            "Epoch 34/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.1885 - accuracy: 0.5242 - val_loss: 2.0227 - val_accuracy: 0.3160\n",
            "Epoch 35/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.1869 - accuracy: 0.5279 - val_loss: 2.0259 - val_accuracy: 0.3152\n",
            "Epoch 36/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.1802 - accuracy: 0.5301 - val_loss: 2.0441 - val_accuracy: 0.3113\n",
            "Epoch 37/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.1736 - accuracy: 0.5325 - val_loss: 2.0373 - val_accuracy: 0.3203\n",
            "Epoch 38/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.1636 - accuracy: 0.5390 - val_loss: 2.0528 - val_accuracy: 0.3179\n",
            "Epoch 39/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.1605 - accuracy: 0.5390 - val_loss: 2.0554 - val_accuracy: 0.3135\n",
            "Epoch 40/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.1537 - accuracy: 0.5423 - val_loss: 2.0773 - val_accuracy: 0.3079\n",
            "Epoch 41/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.1511 - accuracy: 0.5432 - val_loss: 2.0725 - val_accuracy: 0.3094\n",
            "Epoch 42/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.1469 - accuracy: 0.5450 - val_loss: 2.0901 - val_accuracy: 0.3102\n",
            "Epoch 43/50\n",
            "1009/1009 [==============================] - 13s 13ms/step - loss: 1.1440 - accuracy: 0.5453 - val_loss: 2.0805 - val_accuracy: 0.3119\n",
            "Epoch 44/50\n",
            "1009/1009 [==============================] - 15s 15ms/step - loss: 1.1353 - accuracy: 0.5519 - val_loss: 2.1084 - val_accuracy: 0.3156\n",
            "Epoch 45/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.1299 - accuracy: 0.5551 - val_loss: 2.0997 - val_accuracy: 0.3144\n",
            "Epoch 46/50\n",
            "1009/1009 [==============================] - 9s 9ms/step - loss: 1.1250 - accuracy: 0.5560 - val_loss: 2.1100 - val_accuracy: 0.3119\n",
            "Epoch 47/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.1199 - accuracy: 0.5566 - val_loss: 2.1176 - val_accuracy: 0.3127\n",
            "Epoch 48/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.1140 - accuracy: 0.5584 - val_loss: 2.1338 - val_accuracy: 0.3077\n",
            "Epoch 49/50\n",
            "1009/1009 [==============================] - 8s 8ms/step - loss: 1.1149 - accuracy: 0.5615 - val_loss: 2.1248 - val_accuracy: 0.3104\n",
            "Epoch 50/50\n",
            "1009/1009 [==============================] - 7s 7ms/step - loss: 1.1074 - accuracy: 0.5634 - val_loss: 2.1413 - val_accuracy: 0.3076\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b24d036b460>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Separating Validation Data from Training Dataset\n",
        "train_r, val_r, train_m, val_m = train_test_split(train_r, train_m, test_size=0.2, random_state=42)\n",
        "\n",
        "#Class for Matrix Factorisation Model\n",
        "class MatrixFactorization:\n",
        "    #Initialization of class MatrixFactorization with hyperparameters number of latent factors, learning rate and number of training epochs\n",
        "    def __init__(self, n_factors=10, learning_rate=0.01, num_epochs=100):\n",
        "        self.n_factors = n_factors\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    def fit(self, train_r, train_m):\n",
        "        #Declaring self.n_users, self.n_items and self.n_genres from the shape of train_r\n",
        "        self.n_users, self.n_items, self.n_genres = train_r.shape\n",
        "\n",
        "        #Initialization of three matrices with random values which represent latent factors for users, items and genres respectively\n",
        "        self.P = np.random.rand(self.n_users, self.n_factors)\n",
        "        self.Q = np.random.rand(self.n_items, self.n_factors)\n",
        "        self.R = np.random.rand(self.n_genres, self.n_factors)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for i in range(self.n_users):\n",
        "                for j in range(self.n_items):\n",
        "                    for k in range(self.n_genres):\n",
        "                        if train_m[i, j, k] == 1: #Checking if rating exists at [i,j,k] position by looking in binary mask matrix\n",
        "                            eij = train_r[i, j, k] - np.sum(np.dot(self.P[i, :], self.Q[j, :]) * self.R[k, :]) #Calculating the prediction error\n",
        "                            #Updating the latent factors\n",
        "                            for l in range(self.n_factors):\n",
        "                                self.P[i, l] += self.learning_rate * (2 * eij * self.Q[j, l] * self.R[k, l])\n",
        "                                self.Q[j, l] += self.learning_rate * (2 * eij * self.P[i, l] * self.R[k, l])\n",
        "                                self.R[k, l] += self.learning_rate * (2 * eij * self.P[i, l] * self.Q[j, l])\n",
        "\n",
        "    #Function to predict the ratings\n",
        "    def predict(self, data):\n",
        "        user_indices, item_indices, genre_indices = data[:, 0].astype(int), data[:, 1].astype(int), data[:, 2].astype(int)\n",
        "        predictions = np.zeros_like(data[:, 0], dtype=float)\n",
        "        for idx in range(len(user_indices)):\n",
        "            i, j, k = user_indices[idx], item_indices[idx], genre_indices[idx]\n",
        "            predictions[idx] = np.sum(np.dot(self.P[i, :], self.Q[j, :]) * self.R[k, :])\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "KlZsrlyI7TGn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the matrix factorization model and fitting it on the training data\n",
        "model2 = MatrixFactorization(n_factors=10, learning_rate=0.01, num_epochs=20)\n",
        "model2.fit(train_r, train_m)"
      ],
      "metadata": {
        "id": "YeLjOVik8G99"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "6V8wbZid7L6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Making predictions on the validation and test datasets\n",
        "val_pred = model2.predict(val_r)\n",
        "test_pred = model2.predict(test_r)\n",
        "\n",
        "#Defining a threshold to classify as like or not like\n",
        "threshold = 3.5\n",
        "\n",
        "#Initializing variables to keep track of correct and total predictions\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "#Initialization of empty val_errors array to store validation dataset errors for each user-item-genre pair\n",
        "val_errors = []\n",
        "for i in range(val_r.shape[0]):\n",
        "    for j in range(val_r.shape[1]):\n",
        "      for k in range(val_r.shape[2]):\n",
        "        if val_m[i, j, k]: #Checking if rating exists at [i,j,k] by looking in binary mask matrix\n",
        "            actual_rating = val_r[i, j, k]\n",
        "            predicted_rating = val_pred[i]\n",
        "            val_errors.append((actual_rating - predicted_rating) ** 2) #Calculating the squared error between actual and predicted rating\n",
        "            #Classifying the predicted rating as like (1) or not like (0) based on the threshold\n",
        "            predicted_label = 1 if predicted_rating >= threshold else 0\n",
        "\n",
        "            #Comparing the actual label with the predicted label\n",
        "            if actual_rating >= threshold and predicted_label == 1:\n",
        "                correct_predictions += 1\n",
        "            elif actual_rating < threshold and predicted_label == 0:\n",
        "                correct_predictions += 1\n",
        "\n",
        "            total_predictions += 1\n",
        "\n",
        "#Calculating the validation accuracy\n",
        "validation_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "val_rmse = np.sqrt(np.mean(val_errors)) #Calcualting RMSE for validation dataset\n",
        "\n",
        "#Initialization of empty test_errors array to store validation dataset errors for each user-item pair\n",
        "test_errors = []\n",
        "for i in range(test_r.shape[0]):\n",
        "    for j in range(test_r.shape[1]):\n",
        "      for k in range(test_r.shape[2]):\n",
        "        if test_m[i, j, k]: #Checking if rating exists at [i,j,k] by looking in binary mask matrix\n",
        "              actual_rating = test_r[i, j, k]\n",
        "              predicted_rating = test_pred[i]\n",
        "              test_errors.append((actual_rating - predicted_rating) ** 2) #Calculating the squared error between actual and predicted rating\n",
        "\n",
        "test_rmse = np.sqrt(np.mean(test_errors)) #Calcualting RMSE for test dataset\n",
        "\n",
        "#Displaying validation accuracy, validation RMSE and test RMSE\n",
        "print(\"Validation Accuracy:\", validation_accuracy)\n",
        "print(\"Validation RMSE:\", val_rmse)\n",
        "print(\"Test RMSE:\", test_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Crs481jxi5D-",
        "outputId": "765c442e-c9b6-4106-f4d3-09f51ea02394"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5969917334825036\n",
            "Validation RMSE: 1.6002473248403473\n",
            "Test RMSE: 1.554342906088067\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}